import torch.backends.cudnn as cudnn
from torch.autograd import Variable
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import torchvision
import torchvision.transforms as transforms
import lpips
import torchvision.models as models
import torch.nn as nn
import network

import argparse
from tqdm.auto import tqdm
import os
import sys
import time
from importlib.machinery import SourceFileLoader
import numpy as np
import wandb

from utils.utils import *
from utils.Dirt import *
from utils.ROLE import compute_raindrop


class Deeplabv3plusMobilenetFeatureExtractor(nn.Module):
    def __init__(self, layers_to_extract):
        super(Deeplabv3plusMobilenetFeatureExtractor, self).__init__()
        # import pdb; pdb.set_trace()
        self.deeplabv3plus_mobilenet = network.modeling.__dict__['deeplabv3plus_mobilenet'](num_classes=19)
        self.deeplabv3plus_mobilenet.load_state_dict( torch.load('/root/data1/best_deeplabv3plus_mobilenet_cityscapes_os16.pth')['model_state']  )
        # weights = FCN_ResNet50_Weights.DEFAULT
        # self.resnet18 = fcn_resnet50(pretrained_backbone=True, weights=weights)
        # self.resnet18 = models.resnet18(pretrained=True)
        # self.layers = layers

        for param in self.deeplabv3plus_mobilenet.parameters():
            param.requires_grad = False
        
        self.layers_to_extract = layers_to_extract if layers_to_extract is not None else []


    def forward(self, x):
        features = {}

        x = self.deeplabv3plus_mobilenet.backbone.low_level_features(x)

        return x

def compute_dirt_raindrop(image_far, depth, args):
    if np.random.uniform() < 0.5:
        return compute_dirt(image_far, depth, args)
    else:
        return compute_raindrop(image_far, depth, args)

def custom_psf_loss(output_psf, target_radius, device):
    """
    Custom loss function to encourage light intensity to be low at the center
    and higher at the periphery.

    Args:
    - output_psf (torch.Tensor): The PSF tensor generated by the model.
    - target_radius (int): Radius defining the central region to minimize intensity.
    - device (str): Device to perform computations ('cpu' or 'cuda').

    Returns:
    - torch.Tensor: Calculated loss.
    """
    dim = output_psf.shape[-1]  # Assuming square PSF for simplicity
    center = dim // 2
    Y, X = torch.meshgrid(torch.arange(dim), torch.arange(dim), indexing='ij')
    Y, X = Y.to(device), X.to(device)

    # Calculate the distance from the center
    distances = torch.sqrt((Y - center) ** 2 + (X - center) ** 2)
    
    # Create a mask for the center area
    center_mask = (distances < target_radius).float()

    # Calculate loss: high penalty for center, low penalty for periphery
    center_loss = (output_psf * center_mask).sum() / center_mask.sum()
    periphery_loss = (output_psf * (1 - center_mask)).sum() / (1 - center_mask).sum()

    # Combine losses: Minimize center loss and maximize periphery loss
    loss = 4*center_loss - periphery_loss
    return loss


def custom_psf_loss_released(output_psf, device):
    """
    Custom loss function to encourage light intensity to be low at the center
    and higher at the periphery. Instead of separating center and periphery in the binary manner,
    use released weight

    Args:
    - output_psf (torch.Tensor): The PSF tensor generated by the model.
    - target_radius (int): Radius defining the central region to minimize intensity.
    - device (str): Device to perform computations ('cpu' or 'cuda').

    Returns:
    - torch.Tensor: Calculated loss.
    """
    dim = output_psf.shape[-1]  # Assuming square PSF for simplicity
    center = dim // 2
    Y, X = torch.meshgrid(torch.arange(dim), torch.arange(dim), indexing='ij')
    Y, X = Y.to(device), X.to(device)

    # Calculate the distance from the center
    distances = torch.sqrt((Y - center) ** 2 + (X - center) ** 2)
    max_distance = dim//2

    # Apply exponential function to distances, the distances will be the weight applied to the error
    distance_weight = torch.tan((torch.pi)/(max_distance*2) * (distances))
    
    # Calculate loss: high penalty for center, low penalty for periphery
    loss = ((output_psf * distance_weight).sum() / distance_weight.sum()) * 100000
    return loss


def train_step_psf_loss(DOE_phase, optics_optimizer, psf_far, far_depth, near_depth, args, epoch):

    param = args.param
    DOE_train = DOE((1,1,param.R, param.R), param.DOE_pitch, param.material, wvl=param.DOE_wvl, device=args.device)
    DOE_train.set_phase_change(DOE_phase)

    doe_psf_far = []
    doe_psf_near = []

    for depth in far_depth:
        doe_psf_far.append(compute_psf_arbitrary_prop(wvl=param.DOE_wvl, 
                                                      depth=torch.tensor(depth), 
                                                      doe=DOE_train, 
                                                      args=args, 
                                                      propagator=args.propagator, 
                                                      use_lens=args.use_lens, 
                                                      pad=False))

    for depth in near_depth:
        doe_psf_near.append(compute_psf_arbitrary_prop(wvl=param.DOE_wvl, 
                                                       depth=torch.tensor(depth), 
                                                       doe=DOE_train, 
                                                       args=args, 
                                                       propagator=args.propagator, 
                                                       use_lens=args.use_lens, 
                                                       pad=False))

    # Far PSF loss
    far_loss = 0
    for i in range(len(far_depth)):
        far_loss = far_loss + args.l1_criterion(doe_psf_far[i], psf_far[i])
    far_loss = far_loss / len(far_depth)

    # Near PSF loss
    target_radius = param.img_res*1.5
    near_loss = 0
    for i in range(len(near_depth)):
        near_loss = near_loss + custom_psf_loss(doe_psf_near[i], target_radius, args.device)
    near_loss = near_loss / len(near_depth)

    wandb.log({"psf_far_loss": far_loss.item()})
    wandb.log({"psf_near_loss": near_loss.item()})
    # total loss
    total_loss = 1000000*far_loss + 500000*near_loss
    wandb.log({"psf_total_loss": total_loss.item()})

    # log loss
    # print('\n Far Loss: %f, Near Loss: %f' % (100000*far_loss.item(), 50000*near_loss.item()))

    total_loss.backward()
    optics_optimizer.step()
    optics_optimizer.zero_grad()  

    return total_loss.detach()


def train_step_image_loss(args, batch_data, DOE_phase, optics_optimizer, net_optimizer=None, net=None):
    """
    Image restoration loss without using the reconstruction network. 
    """
 
    param = args.param
    DOE_train = DOE((1,1,param.R, param.R), param.DOE_pitch, param.material, wvl=param.DOE_wvl, device=args.device)
    DOE_train.set_phase_change(DOE_phase)

    image_far, _ = batch_data
    image_far = image_far.to(args.device)
    _, _, mask, _, _, _, img_conv, psf_near, psf_far = image_formation(image_far,DOE_train, 
                                                                        args.compute_obstruction, 
                                                                        args, 
                                                                        wvls='RGB' if args.color_PSF else 'design', 
                                                                        pad=True if args.color_PSF else False)

    if args.use_network:
        img_conv = net(img_conv, psf_near, psf_far)

    l1_loss = args.l1_loss_weight * args.l1_criterion(img_conv, image_far)
    masked_loss = args.masked_loss_weight * args.l1_criterion(img_conv*mask, image_far*mask)
    loss = l1_loss + masked_loss

    if args.use_perc_loss:
        perc_loss = torch.mean(args.perceptual_loss_weight * 
                            args.perceptual_criterion(2 * img_conv.to(torch.float32) - 1, 2 * image_far.to(torch.float32) - 1))
    
        loss = l1_loss + masked_loss + perc_loss


    # layers = [0, 1, 2, 3, 4, 5, 6, 7]  # Choose which ResNet layers to use (this example uses layer1, layer2, layer3)
    layers_to_extract = ['low_level_features', 'high_level_features']
    feature_extractor = Deeplabv3plusMobilenetFeatureExtractor(layers_to_extract).to(args.device)
    image_far_features = feature_extractor(image_far.float())
    img_conv_features = feature_extractor(img_conv.float())
    domain_adaptation_loss = 0    
    for layer in layers_to_extract:
        # feat_img = image_far_features[layer]
        # feat_conv = img_conv_features[layer]
        feat_img = image_far_features
        feat_conv = img_conv_features
        domain_adaptation_loss += args.da_loss_weight * args.l1_criterion(feat_conv, feat_img)
    loss += domain_adaptation_loss

    wandb.log({"domain_adaptation_loss": domain_adaptation_loss.item()})

    loss.backward()
    wandb.log({"image_l1_loss": l1_loss.item()})
    wandb.log({"image_masked_loss": masked_loss.item()})
    if args.use_perc_loss:
        wandb.log({"image_perc_loss": perc_loss.item()})
    wandb.log({"image_total_loss": loss.item()})

    print('l1 loss: ', l1_loss, 'masked loss: ', masked_loss, 'perc loss: ', perc_loss if args.use_perc_loss else 'not using perc loss')

    if args.use_network:
        net_optimizer.step()
        net_optimizer.zero_grad() 
    optics_optimizer.step()
    optics_optimizer.zero_grad()  

    return loss.detach()

def train_step_image_psf_loss(batch_data, DOE_phase, optics_optimizer, args):
    """
    Image restoration loss without using the reconstruction network. 
    """
 
    param = args.param
    DOE_train = DOE((1,1,param.R, param.R), param.DOE_pitch, param.material, wvl=param.DOE_wvl, device=args.device)
    DOE_train.set_phase_change(DOE_phase)

    # image loss
    image_far, _ = batch_data
    image_far = image_far.to(args.device)
    if args.color_PSF :
        raise NotImplementedError('PSF loss for color PSF is not implemented')
    _, _, mask, _, _, _, img_conv = image_formation(image_far,DOE_train, args.compute_obstruction, args, wvls='design', pad=False)
    G_l1_loss = args.l1_loss_weight * args.l1_criterion(img_conv, image_far)
    G_masked_loss = args.masked_loss_weight * args.l1_criterion(img_conv*mask, image_far*mask)
    plot_depth_based_psf(DOE_train, args=args, depths=[torch.tensor(3)], wvls='design')

    # psf loss 
    psf_far = psf_thin_lens_far(args, torch.tensor(3), pad=False)
    doe_psf_far = compute_psf_arbitrary_prop(wvl=param.DOE_wvl, 
                                                      depth=torch.tensor(3), 
                                                      doe=DOE_train, 
                                                      args=args, 
                                                      propagator=args.propagator, 
                                                      use_lens=args.use_lens, 
                                                      pad=False)
    far_loss = args.l1_criterion(doe_psf_far, psf_far) * 100000

    wandb.log({"psf_l1_loss": G_l1_loss.item()})
    wandb.log({"psf_masked_loss": G_masked_loss.item()})
    wandb.log({"psf_far_loss": far_loss.item()})
    loss = G_l1_loss + G_masked_loss + far_loss
    wandb.log({"psf_total_loss": loss.item()})
    loss.backward()

    optics_optimizer.step()
    optics_optimizer.zero_grad()  
    return loss.detach()

def train_psf_loss(args):

    writer = SummaryWriter(log_dir=args.result_path + '/runs')

    if args.debug:
        np.random.seed(args.seed)
        torch.manual_seed(args.seed)
        if args.device != 'cpu':
            torch.cuda.manual_seed(args.seed)
        cudnn.benchmark = True
        cudnn.enabled=True
    param = args.param

    # build model and loss
    args.l1_criterion = nn.L1Loss().to(args.device)

    DOE_phase = Variable(param.DOE_phase_init.to(args.device), requires_grad=True)
    optics_optimizer = optim.Adam([DOE_phase], lr=args.optics_lr)

    total_step = 0
    train_loss = 0

    # debugging    
    # far_depth = [5,3,1]
    # near_depth = [0.12,0.08,0.05]
    near_depth = [randuni(param.depth_near_min, param.depth_near_max, 1)[0]] # randomly sample the near-point depth from a range
    far_depth = [randuni(param.depth_far_min, param.depth_far_max, 1)[0]] # randomly sample the far-point depth from a range
    # debugging

    psf_far = []

    for depth in far_depth:
        psf_far.append(psf_thin_lens_far(args, depth, pad=False))

    for epoch in tqdm(range(args.n_epochs), position=0, leave=False):
        step_loss = train_step_psf_loss(DOE_phase, optics_optimizer, psf_far, far_depth, near_depth, args, epoch)

        if total_step%args.log_freq==0:
            eval_loss = evaluate_DOE(DOE_phase, total_step, args)
            writer.add_scalar('Loss/train', eval_loss, total_step)  # Log loss for TensorBoard

        total_step += 1
        train_loss += step_loss
        wandb.log({"psf_train_loss": step_loss})
        print('\n Epoch %d Loss: %f' % (epoch, step_loss))
        print('\n DOE phase shape: ', DOE_phase.shape)

        # save model
        if total_step % args.save_freq == 0:
            torch.save(DOE_phase, os.path.join(args.result_path,'DOE_phase_%03d.pt' % (total_step//args.save_freq)))

        print("optics optimizer lr: ", optics_optimizer.param_groups[0]['lr'])
    
    writer.close()    


def train_image_loss(args):

    writer = SummaryWriter(log_dir=args.result_path + '/runs')

    if args.debug:
        np.random.seed(args.seed)
        torch.manual_seed(args.seed)
        if args.device == 'cuda':
            torch.cuda.manual_seed(args.seed)
        cudnn.benchmark = True
        cudnn.enabled=True
    param = args.param

    transform_train = transforms.Compose([
            transforms.RandomCrop(param.data_resolution,pad_if_needed=True), # Places365 image size varies
            transforms.RandomCrop([param.equiv_crop_size, param.equiv_crop_size],pad_if_needed=True),
            transforms.Resize([param.img_res, param.img_res]),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
        ])

    transform_test = transforms.Compose([
            transforms.RandomCrop(param.data_resolution,pad_if_needed=True), # Places365 image size varies
            transforms.CenterCrop(param.equiv_crop_size),
            transforms.Resize([param.img_res, param.img_res]),
            transforms.ToTensor(),
        ])
    if args.obstruction == 'fence':
        trainset = torchvision.datasets.Places365(
            root=param.dataset_dir, split="train-standard", transform=transform_train)
        testset = torchvision.datasets.Places365(
            root=param.dataset_dir, split="val", transform=transform_test)   
        args.compute_obstruction = compute_fence     
    elif args.obstruction == 'raindrop':
        trainset = torchvision.datasets.ImageFolder(param.training_dir, transform=transform_train)
        testset = torchvision.datasets.ImageFolder(param.val_dir, transform=transform_test)
        args.compute_obstruction = compute_raindrop
    elif args.obstruction == 'dirt':
        trainset = torchvision.datasets.ImageFolder(param.training_dir, transform=transform_train)
        testset = torchvision.datasets.ImageFolder(param.val_dir, transform=transform_test)
        args.compute_obstruction = compute_dirt
    elif args.obstruction == 'dirt_raindrop':
        trainset = torchvision.datasets.ImageFolder(param.training_dir, transform=transform_train)
        testset = torchvision.datasets.ImageFolder(param.val_dir, transform=transform_test)
        args.compute_obstruction = compute_dirt_raindrop

    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True)
    testloader = torch.utils.data.DataLoader(testset, batch_size=args.batch_size, shuffle=False)

    if args.debug: 
        trainloader = testloader

    # build model and loss
    args.l1_criterion = nn.L1Loss().to(args.device)
    
    if args.use_perc_loss:
        args.perceptual_criterion = lpips.LPIPS(net='vgg').to(args.device)

    DOE_phase = torch.tensor(param.DOE_phase_init.to(args.device), requires_grad=True)
    optics_optimizer = optim.Adam([DOE_phase], lr=args.optics_lr)

    if args.use_network:
        from models.RefineNetwork import RefineNetwork
        net = RefineNetwork(args).to(args.device)
        if args.pretrained_network is not None:
            net.load_state_dict(torch.load(args.pretrained_network, map_location='cpu'))
            net.to(args.device)
        net_optimizer = optim.Adam(params=net.parameters(), lr=args.network_lr)
    else:
        net = None 
        net_optimizer = None 

    test_data = next(iter(testloader))

    total_step = 0
    train_loss = 0
    eval_minimum_loss = np.inf

    for epoch in tqdm(range(args.n_epochs), position=0, leave=False):
        for step, batch_data in enumerate(tqdm(trainloader)):
            step_loss = train_step_image_loss(args, batch_data, DOE_phase, optics_optimizer, net_optimizer, net)

            if total_step%args.log_freq==0:
                eval_loss = evaluate_DOE(DOE_phase, total_step, args, net, chromatic_aberration=True if args.color_PSF else False)
                writer.add_scalar('Loss/train', eval_loss, total_step)  # Log loss for TensorBoard
            
            total_step += 1
            train_loss += step_loss
            # os.system('clear')
            print('\n Epoch %d Loss: %f' % (epoch, step_loss))
            print('\n DOE phase shape: ', DOE_phase.shape)
            wandb.log({"image_train_loss": step_loss})

            # save model
            if total_step % args.save_freq == 0:
                torch.save(DOE_phase, os.path.join(args.result_path,'DOE_phase_%03d.pt' % (total_step//args.save_freq)))
                if args.use_network:
                    torch.save(net.state_dict(), os.path.join(args.result_path,'network_%03d.pt' % (total_step//args.save_freq)))

            if eval_loss < eval_minimum_loss:
                eval_minimum_loss = eval_loss
                torch.save(DOE_phase, os.path.join(args.result_path,f'DOE_phase_minimum_eval_loss.pt'))
                if args.use_network:
                    torch.save(net.state_dict(), os.path.join(args.result_path,f'DOE_phase_minimum_eval_loss.pt'))

            # Learning rate scheduling
            print("optics optimizer lr: ", optics_optimizer.param_groups[0]['lr'])

    writer.close()  


def main():
    parser = argparse.ArgumentParser(
        description='PSF based Obstruction-free Metasurface training',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    def none_or_str(value):
        if value == 'None':
            return None
        return value
    
    parser.add_argument('--debug', action="store_true", help='debug mode, train on validation data to speed up the process')
    parser.add_argument('--result_path', default = './example/asset/ckpt/metasurface', type=str, help='dir to save models and checkpoints')
    parser.add_argument('--eval_path', default="dataset/eval", type=str, help='path where an image for evaluation is saved')
    parser.add_argument('--param_file', default= './example/asset/config/param_MV_1600_metasurface.py', type=str, help='path to param file')
    parser.add_argument('--pretrained_DOE', default = None, type=none_or_str, help = 'Directory of pretrained DOE or None')
    parser.add_argument('--pretrained_network', default = None, type=none_or_str, help = 'Directory of pretrained network or None')

    # Related to dataset usage (used for image restoration loss)
    parser.add_argument('--use_dataset', default = False, action="store_true", help='Use dataset for doe training. Used when using image restoration loss')
    parser.add_argument('--use_network', default = False, action="store_true", help='Use refinement network for end-to-end training')
    parser.add_argument('--fence_dataset_dir', help='Directory of dataset used for fence obstruction')
    parser.add_argument('--dirt_raindrop_dataset_train_dir', help='Directory of dataset used for raindrop, dirt, and raindrop_dirt obstructions')
    parser.add_argument('--dirt_raindrop_dataset_val_dir', help='Directory of dataset used for raindrop, dirt, and raindrop_dirt obstructions')

    parser.add_argument('--obstruction', default = 'dirt_raindrop', type = str, help = 'obsturction type')
    parser.add_argument('--propagator', default = 'Fraunhofer', type = str, help = 'propagator used to compute the psf')
    parser.add_argument('--use_lens', action="store_true", help = 'Additional lens usage. Look at compute_psf_arbitrary_prop. Note that Fresnel+Lens is Fraunhofer')
    parser.add_argument('--sensor_noise', default = 0.008, type=float, help='sensor noise level')
    parser.add_argument('--n_epochs', default = 1, type = int, help = 'max num of training epoch')
    parser.add_argument('--optics_lr', default=0.1, type=float, help='optical element learning rate')
    parser.add_argument('--network_lr', default=1e-4, type=float, help='reconstruction network learning rate')

    parser.add_argument('--use_perc_loss', action="store_true", help = 'use lpips perceptual loss')
    parser.add_argument('--l1_loss_weight', default = 1, type = float, help = 'weight for L1 loss')
    parser.add_argument('--masked_loss_weight', default = 1, type = float, help = 'weight for masked loss (focus on obstructed scene)')
    parser.add_argument('--perceptual_loss_weight', default = 1, type = float, help = 'weight for perceptual loss')
    parser.add_argument('--resizing_method', default='area', type=str, help='PSF resizing method. original or area. look at compute_psf function')
    parser.add_argument('--color_PSF', action="store_true", help='If False, use single psf for all color channel. If True, use color-variant psf. Must be true when --use_network option is True, or the dimension difference makes error during network operation')

    parser.add_argument('--log_freq', default=30, type=int, help = 'frequency (num_steps) of logging')
    parser.add_argument('--save_freq', default=400, type=int, help = 'frequency (num_steps) of saving checkpoint and visual performance')
    parser.add_argument('--seed', type=int, default=1234, help='random seed')
    parser.add_argument('--random_init', default=False, action="store_true", help='randomly initialize Metasurface phase')
    parser.add_argument('--device', default='cpu', type=str, help='torch_device')
    parser.add_argument('--batch_size', default=1, type=int, help='image data batch size')
    parser.add_argument('--da_loss_weight', default = 0.5, type = float, help = 'weight for domain adaptation loss')

    args = parser.parse_args()

    param = SourceFileLoader("param", args.param_file).load_module()
    param = convert_resolution(param,args)

    now = datetime.now().strftime('%m-%d-%H-%M')
    run_name = f'deeplabv3plus_metasurface-da_loss-feature-regression_5e-1-{now}'
    wandb.init(project='Diffractive_cloaking',name=f'{run_name}')

    if args.pretrained_DOE is not None:
        param.DOE_phase_init = torch.load(args.pretrained_DOE, map_location=args.device).detach()
    else:
        if args.random_init is True:
            # Random initialization
            param.DOE_phase_init = torch.rand(param.DOE_phase_init.shape, device=args.device) * 10
        else:
            # zero initialization
            param.DOE_phase_init = torch.zeros(param.DOE_phase_init.shape, device=args.device)
            print('ze initialized DOE phase')
        

    save_settings(args, param)

    if args.use_dataset:
        # train using image dataset and image reconstruction loss
        train_image_loss(args)
    else:
        # train using psf loss
        train_psf_loss(args)

if __name__ == '__main__':
    main()
